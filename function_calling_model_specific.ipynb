{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddd65e66-7d04-4f08-bd89-bf1027c5179a",
   "metadata": {},
   "source": [
    "# How to do function calling using InvokeModel API and model-specific prompting \n",
    "\n",
    "This notebook demonstrates how we can use the `InvokeModel API` with external functions to support tool calling. \n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Tool calling with Anthropic Claude 3.5 Sonnet** We demonstrate how to define a single tool. In our case, for simulating a stock ticker symbol lookup tool `get_ticker_symbol` and allow the model to call this tool to return a a ticker symbol.\n",
    "- **Tool calling with Meta Llama 3.3** We modify the prompts to fit Meta's suggested prompt format.\n",
    "\n",
    "## Tool calling with Anthropic Claude 3.5 Sonnet\n",
    "\n",
    "We set our tools and functions through Python functions.\n",
    "\n",
    "We start by defining a tool for simulating a stock ticker symbol lookup tool (`get_ticker_symbol`). Note in our example we're just returning a constant ticker symbol for a select group of companies to illustrate the concept, but you could make it fully functional by connecting it to any stock or finance API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "431698d7-ae6d-4726-aad5-3b5b657236a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3 --quiet\n",
    "!pip install botocore --quiet\n",
    "!pip install beautifulsoup4 --quiet\n",
    "!pip install lxml --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50c9bb9-f3b0-4e7b-9309-f730688a583c",
   "metadata": {},
   "source": [
    "This first example leverages Claude Sonnet 3.5 in the `us-west-2` region. Later, we continue with implementations using various other models available in Amazon Bedrock. The full list of models and supported regions can be found [here](https://docs.aws.amazon.com/bedrock/latest/userguide/models-regions.html). Ensure you have access to the models discussed at the beginning of the notebook. The models are invoked via `bedrock-runtime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e48b403e-0949-433b-9768-56bc0835e025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from bs4 import BeautifulSoup \n",
    "import boto3\n",
    "import json\n",
    "\n",
    "\n",
    "modelId = 'anthropic.claude-3-5-sonnet-20240620-v1:0'\n",
    "region = 'us-west-2'\n",
    "\n",
    "bedrock = boto3.client(\n",
    "    service_name = 'bedrock-runtime',\n",
    "    region_name = region,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3cfb2c-03a0-45ea-8003-aba18f5a7d23",
   "metadata": {},
   "source": [
    "### Helper Functions & Prompt Templates\n",
    "\n",
    "We define a few helper functions and tools that each model uses.\n",
    "\n",
    "First, we define `ToolsList` class with a member function, namely `get_ticker_symbol`, which returns the ticker symbol of a limited set of companies. Note that there is nothing specific to the model used or Amazon Bedrock in these definitions. You can add more functions in the `ToolsList` class for added capabilities (for ex. a function that calls a finance API to retrieve stock information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ddd955b-940c-43df-aa1d-0cb41175d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ticker_symbol(company_name: str) -> str:\n",
    "\n",
    "    if company_name.lower() == \"general motors\":\n",
    "        return 'GM'\n",
    "        \n",
    "    elif company_name.lower() == \"apple\":\n",
    "        return 'AAPL'\n",
    "\n",
    "    elif company_name.lower() == \"amazon\":\n",
    "        return 'AMZN'\n",
    "\n",
    "    elif company_name.lower() == \"3M\":\n",
    "        return 'MMM'\n",
    "\n",
    "    elif company_name.lower() == \"nvidia\":\n",
    "        return 'NVDA'\n",
    "\n",
    "    else:\n",
    "        return 'TickerNotFound'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95677d70-5d6a-41be-b1c0-dd22d9e86555",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Additional tool functions for function calling examples with Bedrock models.\n",
    "\"\"\"\n",
    "\n",
    "def get_us_president_info(president_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns information about a US president based on their name.\n",
    "    \n",
    "    Args:\n",
    "        president_name (str): The name of the US president to look up\n",
    "        \n",
    "    Returns:\n",
    "        str: Information about the president or a message if not found\n",
    "    \"\"\"\n",
    "    presidents_data = {\n",
    "        \"george washington\": {\n",
    "            \"term\": \"1789-1797\",\n",
    "            \"party\": \"No Party\",\n",
    "            \"vice_president\": \"John Adams\",\n",
    "            \"facts\": \"First president of the United States and commander-in-chief during the American Revolutionary War.\"\n",
    "        },\n",
    "        \"abraham lincoln\": {\n",
    "            \"term\": \"1861-1865\",\n",
    "            \"party\": \"Republican\",\n",
    "            \"vice_president\": \"Hannibal Hamlin, Andrew Johnson\",\n",
    "            \"facts\": \"Led the United States through the Civil War and abolished slavery.\"\n",
    "        },\n",
    "        \"franklin d roosevelt\": {\n",
    "            \"term\": \"1933-1945\",\n",
    "            \"party\": \"Democratic\",\n",
    "            \"vice_president\": \"John Nance Garner, Henry A. Wallace, Harry S. Truman\",\n",
    "            \"facts\": \"Only president elected to office four times. Led the US through the Great Depression and most of World War II.\"\n",
    "        },\n",
    "        \"john f kennedy\": {\n",
    "            \"term\": \"1961-1963\",\n",
    "            \"party\": \"Democratic\",\n",
    "            \"vice_president\": \"Lyndon B. Johnson\",\n",
    "            \"facts\": \"Youngest elected president at age 43. Assassinated in Dallas, Texas in 1963.\"\n",
    "        },\n",
    "        \"barack obama\": {\n",
    "            \"term\": \"2009-2017\",\n",
    "            \"party\": \"Democratic\",\n",
    "            \"vice_president\": \"Joe Biden\",\n",
    "            \"facts\": \"First African American president of the United States.\"\n",
    "        },\n",
    "        \"joe biden\": {\n",
    "            \"term\": \"2021-Present\",\n",
    "            \"party\": \"Democratic\",\n",
    "            \"vice_president\": \"Kamala Harris\",\n",
    "            \"facts\": \"Oldest person to assume the presidency.\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Convert to lowercase for case-insensitive matching\n",
    "    president_name = president_name.lower()\n",
    "    \n",
    "    if president_name in presidents_data:\n",
    "        data = presidents_data[president_name]\n",
    "        return f\"President: {president_name.title()}\\nTerm: {data['term']}\\nParty: {data['party']}\\nVice President(s): {data['vice_president']}\\nFacts: {data['facts']}\"\n",
    "    else:\n",
    "        return \"Information about this president is not available in the database.\"\n",
    "\n",
    "def get_weather_data(city: str, date: str = \"today\") -> str:\n",
    "    \"\"\"\n",
    "    Returns sample weather data for a given city and date.\n",
    "    \n",
    "    Args:\n",
    "        city (str): The name of the city to get weather for\n",
    "        date (str, optional): The date to get weather for. Defaults to \"today\".\n",
    "        \n",
    "    Returns:\n",
    "        str: Weather information for the specified city and date\n",
    "    \"\"\"\n",
    "    # Sample weather data for demonstration purposes\n",
    "    weather_data = {\n",
    "        \"new york\": {\n",
    "            \"today\": {\n",
    "                \"temperature\": \"72°F (22°C)\",\n",
    "                \"condition\": \"Partly Cloudy\",\n",
    "                \"humidity\": \"65%\",\n",
    "                \"wind\": \"10 mph NE\"\n",
    "            },\n",
    "            \"tomorrow\": {\n",
    "                \"temperature\": \"75°F (24°C)\",\n",
    "                \"condition\": \"Sunny\",\n",
    "                \"humidity\": \"60%\",\n",
    "                \"wind\": \"8 mph SW\"\n",
    "            }\n",
    "        },\n",
    "        \"london\": {\n",
    "            \"today\": {\n",
    "                \"temperature\": \"62°F (17°C)\",\n",
    "                \"condition\": \"Rainy\",\n",
    "                \"humidity\": \"80%\",\n",
    "                \"wind\": \"15 mph W\"\n",
    "            },\n",
    "            \"tomorrow\": {\n",
    "                \"temperature\": \"64°F (18°C)\",\n",
    "                \"condition\": \"Overcast\",\n",
    "                \"humidity\": \"75%\",\n",
    "                \"wind\": \"12 mph SW\"\n",
    "            }\n",
    "        },\n",
    "        \"tokyo\": {\n",
    "            \"today\": {\n",
    "                \"temperature\": \"81°F (27°C)\",\n",
    "                \"condition\": \"Clear\",\n",
    "                \"humidity\": \"70%\",\n",
    "                \"wind\": \"7 mph SE\"\n",
    "            },\n",
    "            \"tomorrow\": {\n",
    "                \"temperature\": \"83°F (28°C)\",\n",
    "                \"condition\": \"Sunny\",\n",
    "                \"humidity\": \"65%\",\n",
    "                \"wind\": \"5 mph E\"\n",
    "            }\n",
    "        },\n",
    "        \"sydney\": {\n",
    "            \"today\": {\n",
    "                \"temperature\": \"68°F (20°C)\",\n",
    "                \"condition\": \"Partly Cloudy\",\n",
    "                \"humidity\": \"55%\",\n",
    "                \"wind\": \"18 mph S\"\n",
    "            },\n",
    "            \"tomorrow\": {\n",
    "                \"temperature\": \"70°F (21°C)\",\n",
    "                \"condition\": \"Sunny\",\n",
    "                \"humidity\": \"50%\",\n",
    "                \"wind\": \"15 mph SE\"\n",
    "            }\n",
    "        },\n",
    "        \"cairo\": {\n",
    "            \"today\": {\n",
    "                \"temperature\": \"95°F (35°C)\",\n",
    "                \"condition\": \"Hot and Sunny\",\n",
    "                \"humidity\": \"30%\",\n",
    "                \"wind\": \"12 mph N\"\n",
    "            },\n",
    "            \"tomorrow\": {\n",
    "                \"temperature\": \"97°F (36°C)\",\n",
    "                \"condition\": \"Hot and Clear\",\n",
    "                \"humidity\": \"25%\",\n",
    "                \"wind\": \"10 mph NE\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Convert to lowercase for case-insensitive matching\n",
    "    city = city.lower()\n",
    "    date = date.lower()\n",
    "    \n",
    "    if city in weather_data and date in weather_data[city]:\n",
    "        data = weather_data[city][date]\n",
    "        return f\"Weather for {city.title()} ({date}):\\nTemperature: {data['temperature']}\\nCondition: {data['condition']}\\nHumidity: {data['humidity']}\\nWind: {data['wind']}\"\n",
    "    else:\n",
    "        return f\"Weather data for {city.title()} on {date} is not available.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7855da-7b1a-4b47-b102-16ee13d6b104",
   "metadata": {},
   "source": [
    "The models we cover in this notebook support XML or JSON formatting to parse input prompts. We define a simple helper function converting a model's function choice into the XML format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74dbaf5a-760c-41ee-bfa0-9d795e382ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the functions results for input back to the model using XML in its response\n",
    "def func_results_xml(tool_name, tool_return):\n",
    "   return f\"\"\"\n",
    "        <function_results>\n",
    "            <result>\n",
    "                <tool_name>{tool_name}</tool_name>\n",
    "                <stdout>\n",
    "                    {tool_return}\n",
    "                </stdout>\n",
    "            </result>\n",
    "        </function_results>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb304eda-9bcc-42b1-b2ac-9216e841103d",
   "metadata": {},
   "source": [
    "We define a function to parse the model's XML output into readable text. Since each model returns a different response format (i.e. Anthropic Claude's completion can be retrieved by `response['content'][0]['text']` and Meta Llama 3.1 uses `response['generation']`). Further, we create equivalent functions for the other models covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bec46d3f-6782-472c-b775-a8342d69ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parses the output of Claude to extract the suggested function call and parameters\n",
    "def parse_output_claude_xml(response):\n",
    "    soup=BeautifulSoup(response['content'][0]['text'].replace('\\n',''),\"lxml\")\n",
    "    tool_name=soup.tool_name.string\n",
    "    parameter_name=soup.parameters.contents[0].name\n",
    "    parameter_value=soup.parameters.contents[0].string\n",
    "    return (tool_name,{parameter_name:parameter_value})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f5fa44-566b-4822-b876-e82f1b881b50",
   "metadata": {},
   "source": [
    "Without `Converse`, models present some difference in their `InvokeModel API` around their hyperparameters. We define the function to invoke Anthropic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38c79f22-ad81-442c-a994-ecaf4391e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3 invocation function\n",
    "def invoke_anthopic_model(bedrock_runtime, messages, max_tokens=512,top_p=1,temp=0):\n",
    "\n",
    "    body=json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temp,\n",
    "            \"top_p\": top_p,\n",
    "            \"stop_sequences\":[\"</function_calls>\"]\n",
    "        }  \n",
    "    )  \n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(body=body, modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\")\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "    return response_body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35403cb-5587-4e8a-8d1e-6d4f326b6521",
   "metadata": {},
   "source": [
    "### Creating the prompt template\n",
    "\n",
    "We now define the system prompt provided to Claude when implementing function calling including several important components:\n",
    "\n",
    "- An instruction describing the intent and setting the context for function calling.\n",
    "- A detailed description of the tool(s) and expected parameters that Claude can suggest the use of.\n",
    "- An example of the structure of the function call so that it can be parsed by the client code and ran.\n",
    "- A directive to form a thought process before deciding on a function to call.\n",
    "- The user query itself.\n",
    "\n",
    "We supply `get_ticker_symbol` as a tool the model has access to respond to given type of query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "619f24d3-b913-418a-950b-33db8d8b1171",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Updated Claude system prompt that includes the new tools.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = \"\"\"In this environment you have access to a set of tools you can use to answer the user's question.\n",
    "    \n",
    "    You may call them like this:\n",
    "            \n",
    "    <function_calls>\n",
    "    <invoke>\n",
    "    <tool_name>$TOOL_NAME</tool_name>\n",
    "    <parameters>\n",
    "    <$PARAMETER_NAME>$PARAMETER_VALUE</$PARAMETER_NAME>\n",
    "    ...\n",
    "    </parameters>\n",
    "    </invoke>\n",
    "    </function_calls>\n",
    "            \n",
    "    Here are the tools available:\n",
    "    <tools>\n",
    "    <tool_description>\n",
    "    <tool_name>get_ticker_symbol</tool_name>\n",
    "    <description>Gets the stock ticker symbol for a company searched by name. Returns str: The ticker symbol for the company stock. Raises TickerNotFound: if no matching ticker symbol is found.</description>\n",
    "    <parameters>\n",
    "    <parameter>\n",
    "    <n>company_name</n>\n",
    "    <type>string</type>\n",
    "    <description>The name of the company.</description>\n",
    "    </parameter>\n",
    "    </parameters>\n",
    "    </tool_description>\n",
    "    \n",
    "    <tool_description>\n",
    "    <tool_name>get_us_president_info</tool_name>\n",
    "    <description>Returns information about a US president based on their name.</description>\n",
    "    <parameters>\n",
    "    <parameter>\n",
    "    <n>president_name</n>\n",
    "    <type>string</type>\n",
    "    <description>The name of the US president to look up.</description>\n",
    "    </parameter>\n",
    "    </parameters>\n",
    "    </tool_description>\n",
    "    \n",
    "    <tool_description>\n",
    "    <tool_name>get_weather_data</tool_name>\n",
    "    <description>Returns sample weather data for a given city and date.</description>\n",
    "    <parameters>\n",
    "    <parameter>\n",
    "    <n>city</n>\n",
    "    <type>string</type>\n",
    "    <description>The name of the city to get weather for.</description>\n",
    "    </parameter>\n",
    "    <parameter>\n",
    "    <n>date</n>\n",
    "    <type>string</type>\n",
    "    <description>The date to get weather for (today or tomorrow). Optional, defaults to today.</description>\n",
    "    </parameter>\n",
    "    </parameters>\n",
    "    </tool_description>\n",
    "    </tools>\n",
    "            \n",
    "    Come up with a step by step plan for what steps should be taken, what functions should be called and in \n",
    "    what order. Place your thinking between <rationale> tags. Only create this rationale 1 time before \n",
    "    creating any other outputs.\n",
    "            \n",
    "    You will take in any outputs from called functions which will be in <fnr> tags and use \n",
    "    them to further suggests next steps and actions to take.\n",
    "\n",
    "    If the question is unrelated to the tools available, then refuse to answer it and supply the explanation.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac669735-6cfe-4f74-8c9a-fa559a2ff2b4",
   "metadata": {},
   "source": [
    "We use the Messages API covered [here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html). It manages the conversational exchanges between a user and an Anthropic Claude model (assistant). Anthropic trains Claude models to operate on alternating user and assistant conversational turns. When creating a new message, you specify the prior conversational turns with the messages parameter. The model then generates the next Message in the conversation. \n",
    "\n",
    "We prompt the model with a question within the scope of the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1efa04eb-a5ac-4ee0-b84c-a0cf4dfc5eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_list = [{\"role\": 'user', \"content\": [{\"type\": \"text\", \"text\": f\"\"\"\n",
    "    {system_prompt}\n",
    "    Here is the user's question: <question>What is the ticker symbol of General Motors?</question>\n",
    "\n",
    "    How do you respond to the user's question?\"\"\"}]\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0674ebab-0f9a-4241-b7b6-5331ab557237",
   "metadata": {},
   "source": [
    "We previously added `\"</function_calls>\"` to the list of stop sequences letting Claude end its output prior to generating this token representing a closing bracket. Given the query, the model correctly returns its rationale and the selected tool call. Evidently, the output follows the natural language description in the system prompt passed when calling the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72ba3e99-04be-4a65-8c8c-c2e08bba1505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<rationale>\n",
      "To find the ticker symbol for General Motors, I should use the `get_ticker_symbol` tool and provide the company name \"General Motors\" as input. This tool should return the stock ticker symbol for that company.\n",
      "\n",
      "The steps would be:\n",
      "1. Call the `get_ticker_symbol` tool with the parameter `company_name=\"General Motors\"`\n",
      "2. Use the returned ticker symbol to answer the question\n",
      "\n",
      "Since the question is directly related to finding a company's stock ticker symbol, the available tools are sufficient to answer it.\n",
      "</rationale>\n",
      "\n",
      "<function_calls>\n",
      "<invoke>\n",
      "<tool_name>get_ticker_symbol</tool_name>\n",
      "<parameters>\n",
      "<company_name>General Motors</company_name>\n",
      "</parameters>\n",
      "</invoke>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = invoke_anthopic_model(bedrock, messages=message_list)\n",
    "print(response['content'][0]['text'])\n",
    "\n",
    "message_list.append({\n",
    "        \"role\": 'assistant',\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": response['content'][0]['text']}\n",
    "        ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ca46af5-f625-4267-b948-792b75fdd3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_list = [{\"role\": 'user', \"content\": [{\"type\": \"text\", \"text\": f\"\"\"\n",
    "    {system_prompt}\n",
    "    Here is the user's question: <question>Who was George Washington?</question>\n",
    "\n",
    "    How do you respond to the user's question?\"\"\"}]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1077cf27-8a36-4f22-9ae4-b2a25a090f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<rationale>\n",
      "To answer the question \"Who was George Washington?\", the most relevant tool available is get_us_president_info. This tool allows us to look up information about US presidents by name.\n",
      "\n",
      "The steps would be:\n",
      "1. Call the get_us_president_info tool with the parameter president_name=\"George Washington\"\n",
      "2. Use the information returned by the tool to provide details about George Washington's life and presidency.\n",
      "\n",
      "Since the question is specifically about a US president, the other available tools (get_ticker_symbol and get_weather_data) are not relevant for answering this query.\n",
      "</rationale>\n",
      "\n",
      "<function_calls>\n",
      "<invoke>\n",
      "<tool_name>get_us_president_info</tool_name>\n",
      "<parameters>\n",
      "<president_name>George Washington</president_name>\n",
      "</parameters>\n",
      "</invoke>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = invoke_anthopic_model(bedrock, messages=message_list)\n",
    "print(response['content'][0]['text'])\n",
    "\n",
    "message_list.append({\n",
    "        \"role\": 'assistant',\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": response['content'][0]['text']}\n",
    "        ]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4688eb-ca5e-4ee8-86db-20e73cb0e142",
   "metadata": {},
   "source": [
    "## Tool calling with Meta Llama 3.3\n",
    "\n",
    "Now we cover function calling using Meta Llama 3.3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c99471a4-f7f7-4bf6-9073-2d14b170b201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta Llama 3 invocation function\n",
    "bedrock = boto3.client('bedrock-runtime',region_name='us-west-2')\n",
    "\n",
    "def invoke_llama_model(bedrock_runtime, messages, max_tokens=512,top_p=1,temp=0):\n",
    "    \n",
    "    body=json.dumps(\n",
    "        {\n",
    "            \"max_gen_len\": max_tokens,\n",
    "            \"prompt\": messages,\n",
    "            \"temperature\": temp,\n",
    "            \"top_p\": top_p,\n",
    "        }  \n",
    "    )  \n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(body=body, modelId=\"us.meta.llama3-3-70b-instruct-v1:0\")\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "    return response_body\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d2e440-61c2-44e4-a52f-bf47552d9633",
   "metadata": {},
   "source": [
    "We define Llama's system prompt based on Meta's own [documentation](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/#built-in-tooling). We define our custom tools as a JSON dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5a5a1b4-7d55-43c6-99e8-06b7e7c8e82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Updated Llama system prompt that includes the new tools.\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "    <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    Cutting Knowledge Date: December 2023\n",
    "    Today Date: {datetime.today().strftime('%Y-%m-%d')}\n",
    "\n",
    "    When you receive a tool call response, use the output to format an answer to the orginal user question.\n",
    "\n",
    "    You are a helpful assistant with tool calling capabilities.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "    Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\n",
    "    \n",
    "    Respond in the format {{\\\\\\\"name\\\\\\\": function name, \\\\\\\"parameters\\\\\\\": dictionary of argument name and its value}}. Do not use variables.\n",
    "    If the question is unrelated to the tools available, then refuse to answer it and supply the explanation.\n",
    "    \n",
    "    {{\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {{\n",
    "        \"name\": \"get_ticker_symbol\",\n",
    "        \"description\": \"Returns the ticker symbol of a company if a user searches by its company name\",\n",
    "        \"parameters\": {{\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {{\n",
    "            \"company_name\": {{\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The name of the company.\"\n",
    "            }}\n",
    "            }},\n",
    "            \"required\": [\"company_name\"]\n",
    "        }}\n",
    "        }}\n",
    "    }},\n",
    "    {{\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {{\n",
    "        \"name\": \"get_us_president_info\",\n",
    "        \"description\": \"Returns information about a US president based on their name\",\n",
    "        \"parameters\": {{\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {{\n",
    "            \"president_name\": {{\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The name of the US president to look up.\"\n",
    "            }}\n",
    "            }},\n",
    "            \"required\": [\"president_name\"]\n",
    "        }}\n",
    "        }}\n",
    "    }},\n",
    "    {{\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {{\n",
    "        \"name\": \"get_weather_data\",\n",
    "        \"description\": \"Returns sample weather data for a given city and date\",\n",
    "        \"parameters\": {{\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {{\n",
    "            \"city\": {{\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The name of the city to get weather for.\"\n",
    "            }},\n",
    "            \"date\": {{\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The date to get weather for (today or tomorrow). Optional, defaults to today.\"\n",
    "            }}\n",
    "            }},\n",
    "            \"required\": [\"city\"]\n",
    "        }}\n",
    "        }}\n",
    "    }}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dc04f4-f7ac-495b-a2f8-65c51897b9fe",
   "metadata": {},
   "source": [
    "We supply the result to the message and invoke the model to summarize the result. The model correctly summarizes the conversation flow resulting from the initial query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "427df7c7-8252-4128-9a17-a5b170cb37fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"name\": \"get_ticker_symbol\", \"parameters\": {\"company_name\": \"Apple\"}}\n"
     ]
    }
   ],
   "source": [
    "# Call LLama 3.3 and print response\n",
    "message = f\"\"\"{system_prompt}\n",
    "    Question: What is the symbol for Apple?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\"\n",
    "\n",
    "response = invoke_llama_model(bedrock, messages=message)\n",
    "print(response['generation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfee849-5419-4703-9f38-da575dd799d8",
   "metadata": {},
   "source": [
    "Once we have the necessary tool call, we can follow a similar path to other models by executing the function, then returning the result to the model.\n",
    "\n",
    "If asking a question outside the model's scope, the model refuses to answer. It is possible to modify the instructions so the model answers the question by relying on its internal knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9df94107-03f2-43f5-9542-198f602b9def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"name\": \"get_us_president_info\", \"parameters\": {\"president_name\": \"George Washington\"}}\n"
     ]
    }
   ],
   "source": [
    "# Call LLama 3.3 and print response\n",
    "message = f\"\"\"{system_prompt}\n",
    "    Question: Who was George Washington?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\"\n",
    "\n",
    "response = invoke_llama_model(bedrock, messages=message)\n",
    "print(response['generation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0769b565-8b2e-4e8b-ba7f-422e9023c2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"name\": \"get_weather_data\", \"parameters\": {\"city\": \"NYC\"}}\n"
     ]
    }
   ],
   "source": [
    "# Call LLama 3.3 and print response\n",
    "message = f\"\"\"{system_prompt}\n",
    "    Question: Whats the weather in NYC?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\"\n",
    "\n",
    "response = invoke_llama_model(bedrock, messages=message)\n",
    "print(response['generation'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
